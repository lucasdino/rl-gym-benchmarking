env:
  name:                 CartPole-v1
  num_envs:             16
  max_episode_steps:    500    # @ 30 fps
  normalize_obs:        False
  normalize_reward:     False
  is_atari:             False
  stack_samples:        1


algo:
  name:                 distributional_ddqn
  seed:                 42     # Comment out if you want random seeding        
  gamma:                0.95
  lr_start:             4e-4   # Standard is 2.5e-4  
  lr_end:               2e-4
  lr_warmup_env_steps:  100000
  batch_size:           32       
  n_step:               8      # n-step TD estimates
  update_every_steps:   1
  use_action_for_steps_train:   1
  use_action_for_steps_eval:    1
  buffer_size:          100000
  buffer_type:          priority_experience_replay     # {priority_experience_replay, replay_buffer}
  warmup_buffer_size:   5000
  overwrite_target_net_grad_updates:  256   # In terms of 'number grad updates', not samples seen. Num steps would be this * num_envs * update_every_steps
  dist_rl_vmin:         -10
  dist_rl_vmax:         10
  dist_num_atoms:       51


train:
  run_name:         cartpole_rainbowablation_rainbow
  total_env_steps:  200000
  log_interval:     20000
  eval_interval:    20000
  eval_envs:        64
  logging_method:   [console]

  num_seeds:            4
  console_log_train:    True
  # threshold_exit_training_on_eval_score: 500
  # wandb_project: Recommend set this in config.py
  wandb_group:          cartpole_rainbow
  save_result:          True
  save_video_at_end:    False
  save_algo_at_end:     False


sampler:
  name: greedy
  # starting_epsilon: 1.0
  # ending_epsilon: 0.1
  # warmup_steps: 0
  # decay_until_step: 800000


networks:
  q_1:
    network_type: distributional_dueling
    network_args:
      shared_mlp_dims: ["InDims", 128]
      shared_mlp_activations: [relu]
      shared_mlp_layer_type: [Linear]
      shared_mlp_layer_extra_args: [{}]
      valuehead_mlp_dims: ["InDims", 128, "OutDims"]
      valuehead_mlp_activations: [relu, identity]
      valuehead_mlp_layer_type: [NoisyLinear, NoisyLinear]
      valuehead_mlp_layer_extra_args: [{sigma0: 0.7}, {sigma0: 0.7}]
      actionhead_mlp_dims: ["InDims", 128, "OutDims"]
      actionhead_mlp_activations: [relu, identity]
      actionhead_mlp_layer_type: [NoisyLinear, NoisyLinear]
      actionhead_mlp_layer_extra_args: [{sigma0: 0.7}, {sigma0: 0.7}]
      mlp_bias: True

  q_2:
    network_type: distributional_dueling
    network_args:
      shared_mlp_dims: ["InDims", 128]
      shared_mlp_activations: [relu]
      shared_mlp_layer_type: [Linear]
      shared_mlp_layer_extra_args: [{}]
      valuehead_mlp_dims: ["InDims", 128, "OutDims"]
      valuehead_mlp_activations: [relu, identity]
      valuehead_mlp_layer_type: [NoisyLinear, NoisyLinear]
      valuehead_mlp_layer_extra_args: [{sigma0: 0.7}, {sigma0: 0.7}]
      actionhead_mlp_dims: ["InDims", 128, "OutDims"]
      actionhead_mlp_activations: [relu, identity]
      actionhead_mlp_layer_type: [NoisyLinear, NoisyLinear]
      actionhead_mlp_layer_extra_args: [{sigma0: 0.7}, {sigma0: 0.7}]
      mlp_bias: True


inference:
  inference_only: False
  # algo_path: None
  # override_cfg: False