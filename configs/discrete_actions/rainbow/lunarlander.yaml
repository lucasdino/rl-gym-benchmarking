env:
  name:                 LunarLander-v3
  num_envs:             16
  max_episode_steps:    500    # @ 30 fps
  normalize_obs:        False
  normalize_reward:     False
  is_atari:             False
  stack_samples:        4


algo:
  name:                 distributional_ddqn
  seed:                 42     # Comment out if you want random seeding        
  gamma:                0.99
  lr_start:             2.5e-4   # Standard is 2.5e-4  
  lr_end:               2.5e-4
  lr_warmup_env_steps:  100000
  batch_size:           32       
  n_step:               8      # n-step TD estimates
  update_every_steps:   1
  use_action_for_steps_train:   1
  use_action_for_steps_eval:    1
  buffer_size:          200000
  buffer_type:          priority_experience_replay     # {priority_experience_replay, replay_buffer}
  warmup_buffer_size:   5000
  overwrite_target_net_grad_updates:  256   # In terms of 'number grad updates', not samples seen. Num steps would be this * num_envs * update_every_steps
  dist_rl_vmin: -50
  dist_rl_vmax: 50
  dist_num_atoms: 51


train:
  run_name:         lunarlander_distrl
  total_env_steps:  1000000
  log_interval:     100000
  eval_interval:    100000
  eval_envs:        64
  logging_method:   [console]

  num_seeds:            1
  console_log_train:    True
  threshold_exit_training_on_eval_score: 500
  # wandb_project: Recommend set this in config.py
  wandb_group:          lunarlander_distrl
  save_result:          True
  save_video_at_end:    False
  save_algo_at_end:     False


sampler:
  name: epsilon_greedy
  starting_epsilon: 1.0
  ending_epsilon: 0.1
  warmup_steps: 0
  decay_until_step: 800000


networks:
  q_1:
    network_type: distributional_dueling
    network_args:
      shared_mlp_dims: ["InDims", 128]
      shared_mlp_activations: [relu]
      shared_mlp_layer_type: [Linear]
      shared_mlp_layer_extra_args: [{}]
      valuehead_mlp_dims: ["InDims", 128, "OutDims"]
      valuehead_mlp_activations: [relu, identity]
      valuehead_mlp_layer_type: [Linear, Linear]
      valuehead_mlp_layer_extra_args: [{}, {}]
      actionhead_mlp_dims: ["InDims", 128, "OutDims"]
      actionhead_mlp_activations: [relu, identity]
      actionhead_mlp_layer_type: [Linear, Linear]
      actionhead_mlp_layer_extra_args: [{}, {}]
      mlp_bias: True

  q_2:
    network_type: distributional_dueling
    network_args:
      shared_mlp_dims: ["InDims", 128]
      shared_mlp_activations: [relu]
      shared_mlp_layer_type: [Linear]
      shared_mlp_layer_extra_args: [{}]
      valuehead_mlp_dims: ["InDims", 128, "OutDims"]
      valuehead_mlp_activations: [relu, identity]
      valuehead_mlp_layer_type: [Linear, Linear]
      valuehead_mlp_layer_extra_args: [{}, {}]
      actionhead_mlp_dims: ["InDims", 128, "OutDims"]
      actionhead_mlp_activations: [relu, identity]
      actionhead_mlp_layer_type: [Linear, Linear]
      actionhead_mlp_layer_extra_args: [{}, {}]
      mlp_bias: True


inference:
  inference_only: False
  # algo_path: None
  # override_cfg: False